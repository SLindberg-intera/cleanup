'''
Author:         Jacob B Fullerton
Date:           September 25, 2017
Company:        Intera Inc.
Project:        This script was funded in support of the "Composite Analysis (CA), Vadose Zone Facet"
Usage:          Intended for internal use at Intera Inc. to generate boundary condition cards using RET output
Pseudo Code:    The code in general works in the following manner:
                1.  Inputs
                2.  Initialize variables, especially dictionary of points by year
                3.  Using for loop, traverse all years in the selected folder, populating dictionary
                4.  After populating dictionary, export results to STOMP readable boundary condition card
'''
# Version number prints to the outputs
__version__='3.6.4'

import datetime
import geopandas
import os
import pandas
import psutil
import sys
import numpy as np
from copy import deepcopy
from itertools import chain
from os.path import expanduser
from shapely.geometry import Point, Polygon
from itertools import zip_longest

# ----------------------------------------------------------------------------------------------------------------------
# Utility functions and classes


class Params:
    # Historical Simulation End
    his_year = 2018

    # Revegetation cycle year
    rev_year = 2100

    # Barrier failure Year
    bar_year = 2570

    # End of Simulation Year
    eos_year = 12070

    # Duration of steady state simulation
    ss_length = 10000

    # Longest duration of vegetation cycle allowed
    reveg_cycle = 30

    # Barrier rate to preserve
    bar_rate = 0.5

    # Input data from file
    input_recharge = ''
    nij_file = ''
    sij_file = ''
    tops_file = ''
    outdir = ''
    num_cocs = ''

    # Process data to be calculated
    unique_polys=[]
    years = []
    spacing_dict = {}
    i_max = 0
    j_max = 0
    xpctd_rchg = {}
    node_count = 0


def is_int(val):
    try:
        int(val)
        return True
    except:
        return False


def get_int_cols(df):
    # Gets all columns that are an integer value
    int_cols = [int(col) for col in df.columns.tolist() if is_int(col)]
    return int_cols


def df_filter_rows(df, row_param, col_name):
    selection = df.loc[df[col_name] == row_param, :]
    return selection


def get_unique_vals(df, col):
    # Wrapper for obtaining a unique list of values from a dataframe column
    unique_vals = df.loc[:, col].unique().tolist()
    return unique_vals


def list_to_matrix(l, n):
    # Credits to @root on stackoverflow.com for this little function
    # l = list
    # n = number of objects to tuck into 2nd dimension
    return [l[i:i+n] for i in range(0, len(l), n)]


def matrix_to_list(l):
    # l = 2d list (all objects must be lists inside of the master list)
    return list(chain.from_iterable(l))


def get_list_range(l):
    try:
        min_val = min(l)
        max_val = max(l)
        return (max_val - min_val)
    except:
        return None


def check_val_proximity(delta1, delta2, prec=3):
    # Verify the previous and current deltas are nearly equal (to the 3rd decimal place)
    try:
        delta1 = round(delta1, prec)
        delta2 = round(delta2, prec)
        if delta2 != 0:
            if delta1 == delta2:
                return True
    except Exception as e:
            print(e)
    return False


def gen_header(filepath, integration=False):
    # Simple wrapper function for writing the header of a file. Supply the full path to the file
    with open(filepath, 'a+') as file:
        file.write(datetime.date.today().strftime('#RET output created on: (%d, %b, %Y)\n'))
        file.write('#Generated by CA_RET2STOMP.py version: ' + __version__ + '\n')
        file.write('#Source files and locations are as follows:\n')
        file_t = max(os.stat(Params.input_recharge).st_mtime for root,_,_ in os.walk(Params.input_recharge))
        file_t = datetime.datetime.fromtimestamp(file_t).strftime('%b %d, %Y at %H:%M:%S')
        file.write('#Recharge Estimates Directory: ' + Params.input_recharge + '; Last modified: ' + file_t + '\n')
        file_t = os.path.getmtime(Params.nij_file)
        file_t = datetime.datetime.fromtimestamp(file_t).strftime('%b %d, %Y at %H:%M:%S')
        file.write('#NIJ File: ' + Params.nij_file + '; Last modified: ' + file_t + '\n')
        file_t = os.path.getmtime(Params.sij_file)
        file_t = datetime.datetime.fromtimestamp(file_t).strftime('%b %d, %Y at %H:%M:%S')
        file.write('#SIJ File: ' + Params.sij_file + '; Last modified: ' + file_t + '\n')
        file_t = os.path.getmtime(Params.tops_file)
        file_t = datetime.datetime.fromtimestamp(file_t).strftime('%b %d, %Y at %H:%M:%S')
        file.write('#TOP File: ' + Params.tops_file + '; Last modified: ' + file_t + '\n')
        file.write('#Output Directory: ' + Params.outdir + '\n')
        file.write('#Number of COCs: ' + str(Params.num_cocs) + '\n')
        if not integration:
            file.write(str(len(Params.unique_polys)) + ',\n')


def calc_node_geom(return_type):
    '''
    :param return_type: A string value representing the desired type of dictionary, "geometries" or "dimensions"
    :var i_max: Integer value representing the maximum node count in the "i" direction
    :var j_max: Integer value representing the maximum node count in the "j" direction
    :return:
    '''
    i_max = Params.i_max
    j_max = Params.j_max
    # Generates geometries depending on the type of dictionary desired
    geom_dict = {}
    if return_type == 'geometries':
        for i in range(1, i_max + 1):
            for j in range(1, j_max + 1):
                # Get cell spacing in the x-dir
                id = str(i) + '_' + str(j)
                id_xdelta = str(i + 1) + '_' + str(j)
                id_ydelta = str(i) + '_' + str(j + 1)
                geom_dict[id] = Polygon([
                    (Params.spacing_dict[id]['X'], Params.spacing_dict[id]['Y']),
                    (Params.spacing_dict[id_xdelta]['X'], Params.spacing_dict[id]['Y']),
                    (Params.spacing_dict[id_xdelta]['X'], Params.spacing_dict[id_ydelta]['Y']),
                    (Params.spacing_dict[id]['X'], Params.spacing_dict[id_ydelta]['Y'])
                ])
    elif return_type == 'dimensions':
        for i in range(1, i_max + 1):
            for j in range(1, j_max + 1):
                # Get cell spacing in the x-dir
                id = str(i) + '_' + str(j)
                id_xdelta = str(i + 1) + '_' + str(j)
                id_ydelta = str(i) + '_' + str(j + 1)
                xdelta = Params.spacing_dict[id_xdelta]['X'] - Params.spacing_dict[id]['X']
                ydelta = Params.spacing_dict[id_ydelta]['Y'] - Params.spacing_dict[id]['Y']
                area = xdelta * ydelta
                geom_dict[id] = {'delx': xdelta, 'dely': ydelta, 'area': area}
    return geom_dict


def apply_grid_geoms(bc_gdf):
    grid_surfs = deepcopy(bc_gdf)
    geom_dict = calc_node_geom('geometries')
    grid_surfs['I_J'] = grid_surfs.I.astype(str) + '_' + grid_surfs.J.astype(str)
    grid_surfs.geometry = grid_surfs['I_J'].map(geom_dict)
    return grid_surfs

# ----------------------------------------------------------------------------------------------------------------------
# Primary functions


def get_parameters():
    # Define user input function in the event that the input instructions are not provided in a text file (or if the
    # values provided by the script input file are incorrect).
    def user_input():
        while True:
            Params.input_recharge = input("Please type the location of the recharge estimate shapefiles:\n")
            if os.path.exists(Params.input_recharge):
                break
        while True:
            Params.nij_file = input("\nPlease type the location and name of the *.nij file:\n")
            if os.path.exists(Params.nij_file):
                break
        while True:
            Params.sij_file = input("\nPlease type the location and name of the *.sij file:\n")
            if os.path.exists(Params.sij_file):
                break
        while True:
            Params.tops_file = input("\nPlease type the location and name of the *.top file:\n")
            if os.path.exists(Params.tops_file):
                break
        while True:
            Params.outdir = input("\nPlease type the location for the output directory:\n")
            if os.path.exists(Params.outdir):
                print("The path already exists, please type in a unique folder to create")
            else:
                try:
                    os.mkdir(Params.outdir)
                    if os.path.exists(Params.outdir):
                        break
                except:
                    pass
        while True:
            num_cocs = input("\nPlease type the number of COCs being modeled:\n")
            try:
                num_cocs = int(num_cocs)
                break
            except:
                print("The value provided for the constituents must be an integer.")
        return Params.input_recharge, Params.nij_file, Params.sij_file, Params.tops_file, Params.outdir, num_cocs
    # Props to @user13993 and @Nisan.H on stackoverflow.com for this elegant answer. Will recognize containing directory
    # of this script and assign it to the 'folder' variable.
    folder = os.path.dirname(os.path.realpath(__file__))
    Params.input_recharge = ''
    Params.nij_file = ''
    Params.sij_file = ''
    Params.tops_file = ''
    Params.outdir = ''
    ### User Input Start ###
    try:
        if sys.argv[1] != '':
            inst_file = sys.argv[1]
        try:
            if not os.path.exists(inst_file):
                print("\nAttempting to use parent directory of the script.")
                print("\n" + folder)
                inst_file = os.path.join(folder, inst_file)
                if not os.path.isfile(inst_file) and not os.path.exists(inst_file):
                    print("\nSorry, the path/file you provided doesn't exist. Try again.\n")
                else:
                    pass
            else:
                if not os.path.isfile(inst_file):
                    print("\nSorry, the path/file you provided doesn't exist. Try again.\n")
                else:
                    pass
        except:
            print("The first input provided should only include a file path to the input file for this script.\n"
                  "The input provided has an error in it.")
        with open(inst_file, 'r') as file:
            Params.input_recharge = file.readline().replace('\n', '').replace('\r','').replace('~', expanduser('~'))
            Params.nij_file = file.readline().replace('\n', '').replace('\r','').replace('~', expanduser('~'))
            Params.sij_file = file.readline().replace('\n', '').replace('\r','').replace('~', expanduser('~'))
            Params.tops_file = file.readline().replace('\n', '').replace('\r','').replace('~', expanduser('~'))
            Params.outdir = file.readline().replace('\n', '').replace('\r','').replace('~', expanduser('~'))
            if os.path.exists(Params.outdir):
                raise Exception("The destination folder already exists, please change your input file.")
            else:
                try:
                    os.mkdir(Params.outdir)
                    if os.path.exists(Params.outdir):
                        pass
                except:
                    raise Exception("Something went wrong when trying to create the new output folder. Check your input file.")
            Params.num_cocs = file.readline().replace('\n', '').replace('\r','')
            try:
                Params.num_cocs = int(Params.num_cocs)
            except:
                Params.num_cocs == False
    except Exception as e:
        print(e)
        exit()
    except:
        inst_file = input('Do you have an instruction file for this script? (y/n):\n')
        try:
            if inst_file.lower() == 'y' or inst_file.lower() == 'yes':
                while True:
                    inst_file = input('\nLocation and name of the file (e.g. /home/documents/myfile.txt):\n')
                    if not os.path.exists(inst_file):
                        print("\nAttempting to use parent directory of the script.")
                        print("\n" + folder)
                        inst_file = os.path.join(folder, inst_file)
                        if not os.path.isfile(inst_file) and not os.path.exists(inst_file):
                            print("\nSorry, the path/file you provided doesn't exist. Try again.\n")
                        else:
                            break
                    else:
                        if not os.path.isfile(inst_file):
                            print("\nSorry, the path/file you provided doesn't exist. Try again.\n")
                        else:
                            break
                with open(inst_file,'r') as file:
                    Params.input_recharge = file.readline().replace('\n', '').replace('\r','').replace('~', expanduser('~'))
                    Params.nij_file = file.readline().replace('\n', '').replace('\r','').replace('~', expanduser('~'))
                    Params.sij_file = file.readline().replace('\n', '').replace('\r','').replace('~', expanduser('~'))
                    Params.tops_file = file.readline().replace('\n', '').replace('\r','').replace('~', expanduser('~'))
                    Params.outdir = file.readline().replace('\n', '').replace('\r','').replace('~', expanduser('~'))
                    if os.path.exists(Params.outdir):
                        raise Exception("The destination folder already exists, please change your input file.")
                    else:
                        try:
                            os.mkdir(Params.outdir)
                            if os.path.exists(Params.outdir):
                                pass
                        except:
                            raise Exception("Something went wrong when trying to create the new output folder. Check your input file.")
                    Params.num_cocs = file.readline().replace('\n', '').replace('\r','')
                    try:
                        Params.num_cocs = int(Params.num_cocs)
                    except:
                        Params.num_cocs == False
        except Exception as e:
            print(e)
            exit()
        except:
            print("Please check your input file. Contact the code administrator if the problem persists.")
            exit()
    try:
        if not os.path.exists(Params.input_recharge) or not os.path.exists(Params.nij_file) or not os.path.exists(Params.tops_file) \
                or not os.path.exists(Params.outdir) or inst_file.lower() == 'n' or inst_file == 'no' or Params.num_cocs == False:
            if not os.path.exists(Params.outdir):
                try:
                    os.mkdir(Params.outdir)
                except:
                    Params.input_recharge, Params.nij_file, Params.sij_file, Params.tops_file, Params.outdir, Params.num_cocs = user_input()
            else:
                Params.input_recharge, Params.nij_file, Params.sij_file, Params.tops_file, Params.outdir, Params.num_cocs = user_input()
    except FileExistsError as e:
        print(e)
        exit()

    print("Obtained parameters and finished setting environment variables")

    with open(os.path.join(Params.outdir, "mem_log.txt"), 'w+') as file:
        s_time = datetime.datetime.now()
        pid = os.getpid()
        py = psutil.Process(pid)
        file.write("Process, Time_Elapsed, Year, Num_Nodes, Memory(MB)\n")

    # The first step is to read in the *.nij and *.top files and generate the nodes as a dictionary of points
    grid_dict = {}

    with open(Params.nij_file, 'r') as file_ij:
        with open(Params.tops_file, 'r') as file_k:
            try:
                node_count = file_ij.readline().replace('\n', '').replace('\r','')
                Params.node_count = np.array([int(num) for num in node_count.split()]).prod()
                Params.i_max, Params.j_max = list(map(int, node_count.split()))
            except:
                print("The first line of the nij file should only have the number of rows and the number of columns.")
                exit()
            for row in file_ij:
                row.replace('\n', '').replace('\r','')
                row = row.split()
                i,j = row[0], row[1]
                key = str(i + '_' + j)
                if key not in grid_dict:
                    grid_dict[key] = {'X': row[2], 'Y': row[3], 'I': i, 'J': j, 'K': ''}
                else:
                    print('Your file has multiple instances of the same location. You cannot have this file work with ' +
                          'this script.')
                    exit()
            for row in file_k:
                row.replace('\n', '').replace('\r','')
                row = row.split()
                i,j = row[0], row[1]
                key = str(i + '_' + j)
                if key in grid_dict:
                    grid_dict[key]['K'] = row[2]
                else:
                    print('Your *.top file does not match your *.nij file. Please verify your file inputs.')
                    exit()

    with open(Params.sij_file, 'r') as file_s:
        next(file_s)
        eastings = []
        northings = []
        for row in file_s:
            row = row.split()
            id = str(row[0]) + '_' + str(row[1])
            if id not in Params.spacing_dict:
                east = float(row[2])
                north = float(row[3])
                Params.spacing_dict[id] = {'X': east, 'Y': north}
                eastings.append(east)
                northings.append(north)
            else:
                print("Error!! Check your spacing file, duplicate id found (i_j): " + id)
                exit()

    # Create a shapefile of the model boundary for getting the expected total recharge based on the polygons
    model_bndry = Polygon([
        [min(eastings), min(northings)], [max(eastings), min(northings)],
        [max(eastings), max(northings)], [min(eastings), max(northings)]
                           ])
    model_bndry = geopandas.GeoDataFrame(geometry=[model_bndry])
    # Create a shapefile of points in memory. Credit to @Dirk on gis.stackexchange.com for this solution
    grid_list = []
    for key in grid_dict:
        grid_list.append(grid_dict[key])
    grid_nodes = pandas.DataFrame(grid_list)
    grid_nodes['geometry'] = grid_nodes.apply(lambda x: Point((float(x.X), float(x.Y))), axis=1)
    grid_nodes = geopandas.GeoDataFrame(grid_nodes, geometry= 'geometry')
    del grid_list

    # Walk through the folder given for the recharge estimates and create a list of the paths for each shapefile with
    # the keyword 'recharge' in the name of the shapefile. Credit to @Martijn Pieters on stackoverflow.com
    recharge_list = []
    for root, dirs, files in os.walk(Params.input_recharge):
        for file in files:
            if 'recharge' in file.lower() and '.shp' in file.lower() and not 'xml' in file.lower() and not 'lock' in file.lower():
                recharge_list.append(os.path.join(root, file))

    with open(os.path.join(Params.outdir, 'error.log'), 'w+') as file:
        file.write(datetime.date.today().strftime('RET process error log (%d, %b, %Y)\n\n'))
        file.write('#Generated by CA_RET2STOMP.py version: ' + __version__ + '\n')
        file.write('Source files and locations are as follows:\n')
        file.write('Recharge Estimates Directory: ' + Params.input_recharge + '\n')
        file_t = os.path.getmtime(Params.nij_file)
        file_t = datetime.datetime.fromtimestamp(file_t).strftime('%b %d, %Y at %H:%M:%S')
        file.write('#NIJ File: ' + Params.nij_file + '; Created on ' + file_t + '\n')
        file_t = os.path.getmtime(Params.sij_file)
        file_t = datetime.datetime.fromtimestamp(file_t).strftime('%b %d, %Y at %H:%M:%S')
        file.write('#SIJ File: ' + Params.sij_file + '; Created on ' + file_t + '\n')
        file_t = os.path.getmtime(Params.tops_file)
        file_t = datetime.datetime.fromtimestamp(file_t).strftime('%b %d, %Y at %H:%M:%S')
        file.write('#TOP File: ' + Params.tops_file + '; Created on ' + file_t + '\n')
        file.write('Output Directory: ' + Params.outdir + '\n\n')

    # Perform a spatial join on the recharge estimates using the nodes geodataframe
    # Process 1

    for year in sorted(recharge_list):
        # Make sure that if the cpg file is not compatible with geopandas to rename it for the load, then revert it back
        try:
            recharge = geopandas.read_file(year)
        except:
            cpg_check = year.replace('.shp', '.cpg')
            if os.path.isfile(cpg_check):
                new_name = cpg_check.replace('.cpg', '.1cpg')
                os.rename(cpg_check, new_name)
            recharge = geopandas.read_file(year)
            os.rename(new_name, cpg_check)
        # Limit geoprocessing to only those features within the boundary
        # Exclude slivers and correct any erroneous geometry by buffering (buff distances = 0.01 m)
        xmin, ymin, xmax, ymax = model_bndry.total_bounds
        recharge = recharge.cx[xmin:xmax, ymin:ymax]
        recharge = recharge.loc[recharge.geometry.area > 0.01, :]
        recharge['geometry'] = recharge.buffer(0.1)
        year = year.split('_').pop().replace('.shp','')
        with open(os.path.join(Params.outdir, "mem_log.txt"), 'a+') as file:
            file.write("1, " + str(datetime.datetime.now() - s_time) + ", " + str(year) + ', ' +
                       str(len(grid_nodes)) + ', ' + str(py.memory_info().vms/1024**2) + '\n')
        if grid_nodes.crs == None:
            grid_nodes.crs = recharge.crs
        # Get the total recharge estimate over the area
        data = []
        for index, row in recharge.iterrows():
            if model_bndry.intersects(row['geometry'])[0]:
                data.append({'geometry': model_bndry.intersection(row['geometry']), 'recharge': row['RechargeRa']})
        if len(data) > 0:
            poly_rchg = geopandas.GeoDataFrame(data)
            poly_rchg['area'] = poly_rchg['geometry'].area
            poly_rchg['rchg_vol(m^3)'] = poly_rchg['area'] * poly_rchg['recharge'] / 1000
            Params.xpctd_rchg[year] = sum(poly_rchg['rchg_vol(m^3)'].values)[0]
        else:
            with open(os.path.join(Params.outdir, 'error.log'), 'a') as file:
                file.write('The SIJ file is not matching with the input shapefile: year = ' + year + '\n')
        # Prepare the points shapefile for scanning the model nodes over the year
        source_col = [col for col in recharge.columns.values if 'source' in col.lower()][0]
        yrly_src = str(year) + r'_src'
        recharge_col = [col for col in recharge.columns.values if 'recharge' in col.lower()][0]
        recharge = recharge.loc[:, [recharge_col, source_col, 'geometry']]
        grid_nodes = geopandas.sjoin(grid_nodes, recharge, how='inner', op='intersects')
        grid_nodes['ID'] = grid_nodes['index_right']
        if yrly_src not in grid_nodes:
            grid_nodes[yrly_src] = grid_nodes[source_col]
        else:
            print("Verify your input RET shapefiles, there's a year that is duplicated {}. ".format(year) +
                  "Please look at the error.log. Skipping duplicate year.")
            with open(os.path.join(Params.outdir, 'error.log'), 'a+') as file:
                file.write("The year in question is: " + year + '\n')
                for item in list(set(grid_nodes['ID'].values).difference(set(grid_nodes['index_right']))):
                    file.write(str(item) + '\n')
            continue
        grid_nodes[year] = grid_nodes[recharge_col]
        grid_nodes.drop([recharge_col, 'index_right'], axis=1, inplace=True)
        if source_col + '_left' in grid_nodes:
            grid_nodes.drop([source_col + '_left'], axis=1, inplace=True)
        if source_col + '_right' in grid_nodes:
            grid_nodes.drop([source_col + '_right'], axis=1, inplace=True)
        if source_col in grid_nodes:
            grid_nodes.drop([source_col], axis=1, inplace=True)
        grid_nodes = grid_nodes.reset_index(drop=True)

        # Cleanup grid_nodes dataset to remove any duplicates
        grid_nodes = clean_duplicates(grid_nodes)

        # Ensure that all important features have been captured in the scan, override features to honor longest dimension
        # for missed features with high recharge
        grid_nodes, grid_surfs = restore_key_features(grid_nodes, recharge, year)

    if not os.path.exists(os.path.join(Params.outdir)):
        os.makedirs(os.path.join(Params.outdir))

    grid_nodes = gen_ts_ids(grid_nodes)
    grid_surfs = apply_grid_geoms(grid_nodes)

    return grid_nodes, recharge_list, grid_surfs


def restore_key_features(grid_nodes, recharge, year):
    # This function is intended to sift through and find all of the polygons that were missed with the node-centered
    # scan, then reincorporate the key features by overriding lesser features. The following process will be used:
    # 1.    Create a shapefile of surfaces representing the STOMP grid
    # 2.    Join the attributes of the recharge estimates shapefile to the surface shapefile
    # 3.    Make sets of the current polygons used and of all polygons, then obtain the difference in sets
    #           to obtain the polygons that were missed (filter the geodataframe by this "missed" list)
    # 4.    Obtain a list of those missed polygons whose values were larger than the node-centered scan
    # 5.    Iterate through the list and update recharge value to that of the missed recharge feature and change the
    #           polygon ID to match that of the missed feature ID
    # 1
    grid_surfs = apply_grid_geoms(grid_nodes)

    # 2
    rchg_nodes_jnd = geopandas.sjoin(grid_surfs, recharge, how='inner', op='intersects')
    rchg_nodes_jnd['rchg_est_ID'] = rchg_nodes_jnd['index_right']
    rchg_nodes_jnd.drop(['index_right'], axis=1, inplace=True)

    # 3
    node_polys = grid_surfs.ID.unique().tolist()
    rchg_polys = rchg_nodes_jnd.rchg_est_ID.unique().tolist()
    missed_polys = list(set(rchg_polys) - set(node_polys))
    rchg_nodes_fltrd = rchg_nodes_jnd.loc[rchg_nodes_jnd.rchg_est_ID.isin(missed_polys)]

    # 4
    polys_to_restore = rchg_nodes_fltrd.loc[rchg_nodes_fltrd.RechargeRa > rchg_nodes_fltrd[year]].rchg_est_ID.unique().tolist()
    # For debugging reasons, print to a file
    with open(os.path.join(Params.outdir, 'polys_to_restore.txt'), 'a+') as file:
        file.write("For year: " + str(year) + "\n")
        file.write("\n".join(list(map(str, polys_to_restore))))
        file.write('\n')

    # Check if polygons are barriers before making adjustments. If the source is a barrier, then ignore the surrounding
    # high recharge features and honor the barrier recharge value
    if len(polys_to_restore) > 0:
        with open(os.path.join(Params.outdir, 'error.log'), 'a+') as file:
            write_str = ['I','J',year + '_Old_Val','Restore_ID','Restore_Value']
            format_str = ''.join(['{' + str(i) + ':<15}' for i in range(len(write_str))])
            file.write('The following nodes were adjusted to reflect high recharge features\npreviously missed by '+
                       'node-centered scan for year: ' + year + '\n' + format_str.format(*write_str) + '\n')
        for poly_restore in sorted(polys_to_restore):
            nodes_to_adjust = rchg_nodes_fltrd.loc[rchg_nodes_fltrd.rchg_est_ID == poly_restore, 'I_J'].tolist()
            rchg_restore_poly = rchg_nodes_fltrd.loc[rchg_nodes_fltrd.rchg_est_ID == poly_restore]
            for node_adjust in nodes_to_adjust:
                i = node_adjust.split('_')[0]
                j = node_adjust.split('_')[-1]
                new_val = rchg_restore_poly.loc[rchg_restore_poly.I_J == node_adjust, 'RechargeRa'].values[0]
                new_id = str(rchg_restore_poly.loc[rchg_restore_poly.I_J == node_adjust, 'rchg_est_ID'].values[0])
                old_val = grid_nodes.loc[(grid_nodes.I == i) & (grid_nodes.J == j), year].values[0]
                if old_val >= new_val:
                    continue
                # Skip nodes which are the barrier recharge rate (honoring node-centers only)
                elif old_val == Params.bar_rate:
                    continue
                grid_nodes.loc[(grid_nodes.I == i) & (grid_nodes.J == j),year] = new_val
                # Change ID of node to be its own unique file. Verify that the ID doesn't already exist
                old_id = str(grid_nodes.loc[(grid_nodes.I == i) & (grid_nodes.J == j),'ID'].values[0])
                if new_id not in old_id:
                    rev_id = "_".join([str(grid_nodes.loc[(grid_nodes.I == i) & (grid_nodes.J == j),'ID'].values[0]),
                                  str(new_id)])
                    grid_nodes.loc[(grid_nodes.I == i) & (grid_nodes.J == j),"ID"] = rev_id

                with open(os.path.join(Params.outdir, 'error.log'), 'a+') as file:
                    write_str = [i, j, old_val, str(int(new_id) + 1), new_val]
                    file.write(format_str.format(*write_str) + '\n')
    return grid_nodes, grid_surfs


def clean_duplicates(grid_nodes):
    # Establish prioritization function
    priority_list = [
        'IDF_Zones',
        'AAX_Zones',
        'WMAC_Zones',
        'ERDF_Zones',
        'Barrier',
        'ehsit',
        'bggensit',
        'bggenexs',
        'AAC_1943',
        'BRMP_2011',
        'DefaultCover',
        None
    ]

    def priority_index(src):
        return priority_list.index(src)

    # Obtain the recharge years to be checked
    years = []
    for col in grid_nodes.columns:
        try:
            years.append(int(col))
        except:
            continue

    # Make a copy of the original dataset, identify priority of sources as indices,
    # create a joint "I_J" column, identify duplicates, initialize a column to mark rows for deletion
    copy_nodes = deepcopy(grid_nodes)
    # Make a list of the sources and create new columns based on indexed priority
    for year in years:
        copy_nodes[str(year) + '_src_indx'] = copy_nodes[str(year) + '_src'].apply(priority_index)
    copy_nodes['I_J'] = copy_nodes.I.astype(str) + '_' + copy_nodes.J.astype(str)
    copy_nodes['Duplicate'] = copy_nodes.duplicated(subset='I_J', keep=False)
    copy_nodes['Remove'] = False
    duplicates = copy_nodes.loc[copy_nodes.Duplicate == True]

    # Iterate over the "I_J" values and determine if they are unique or identical.
    # 1.    If identical in all aspects except polygon ID, verify that polygon ID doesn't exist elsewhere. If not used,
    #       mark for deletion.
    # 2.    If not identical for all years, select based on source (choosing in order of priority as dictated in RET)
    # 3.    Modify the first index to apply best sources by year with corresponding values. Where sources are identical,
    #       choose the higher recharge rate
    nodes = duplicates.I_J.unique().tolist()
    del duplicates
    source_cols = [str(year) + '_src' for year in years]
    src_indices = [src + '_indx' for src in source_cols]
    for node in nodes:
        selection = copy_nodes.loc[(copy_nodes.I_J == node) & (copy_nodes.Remove == False)]
        # This will identify all duplicates that are exactly identical (for recharge years only)
        copy_nodes.loc[copy_nodes.I_J == node, 'Remove'] = selection.index.map(
            selection.duplicated(subset=list(map(str, years))))
        # Reconstruct selection to remove those nodes identified for removal in the previous step
        selection = copy_nodes.loc[(copy_nodes.I_J == node) & (copy_nodes.Remove == False)]
        selection_srtd = selection.sort_values(src_indices + list(map(str, years)), ascending=True).iloc[1:]
        # selection_srtd = selection.sort_values(str(src_indices[-1]), ascending=True)
        # highest_indx = selection_srtd[src_indices[-1]].values[0]
        # if len(selection_srtd[src_indices[-1]] == highest_indx) > 1:
        #     selection_srtd = selection_srtd.loc[selection_srtd[src_indices[-1]] == highest_indx]
        #     remove_duplicate_srcs = selection_srtd.sort_values(list(map(str, years)), ascending=False).iloc[1:]
        #     copy_nodes.loc[copy_nodes.index[remove_duplicate_srcs.index], 'Remove'] = True
        # selection_srtd = selection_srtd.iloc[1:]
        copy_nodes.loc[copy_nodes.index[selection_srtd.index], 'Remove'] = True
        # Check that the nodes have been properly marked for removal without a duplicate, modify node ID to show it has
        # been picked out of the duplicates
        selection = copy_nodes.loc[(copy_nodes.I_J == node) & (copy_nodes.Remove == False)]
        if len(selection) > 1:
            print("Duplicates could not be removed, please check the following node location: " + str(node))
            exit()
        if '_sel' not in selection.ID.astype(str).values[0]:
            copy_nodes.loc[copy_nodes.index[selection.index], 'ID'] = selection.ID.astype(str) + '_sel'
    grid_nodes = deepcopy(copy_nodes.loc[copy_nodes.Remove == False])
    grid_nodes.drop(src_indices + ['I_J', 'Duplicate', 'Remove'], axis=1, inplace=True)
    grid_nodes.reset_index(drop=True, inplace=True)
    return grid_nodes


def gen_externals(bc_gdf):
    '''
    # This function generates the external files used by the STOMP input card.
    # Writes out a text file containing all of the node definitions for each unique polygon/node-grouping as individual
    # files.
    :param bc_gdf: Accepts a geodataframe (generated by GeoPandas)
    :return counter: Returns a count of the number of nodes recorded, which will be used later in a QA checking step
    '''
    counter = 0
    for poly in Params.unique_polys:
        selection = bc_gdf[bc_gdf['ID'] == poly]
        if not os.path.exists(os.path.join(Params.outdir)):
            os.makedirs(os.path.join(Params.outdir))
        with open(os.path.join(Params.outdir, str(poly) + '.dat'), 'w+') as file:
            for index, row in selection.iterrows():
                k = row['K']
                j = row['J']
                i = row['I']
                file.write(i + '\t' + j + '\t' + k + '\t' + '3\n')
                counter += 1
    return counter


def gen_tr_file(bc_gdf):
    filepath = os.path.join(Params.outdir, 'ca_tr_boundary_card.dat')
    gen_header(filepath)
    years = list(map(str, Params.years))
    for poly in Params.unique_polys:
        with open(filepath, 'a+') as file:
            file.write(gen_input_text(bc_gdf, poly, years))
    print("Finished steady transient boundary condition card.")
    return filepath


def gen_input_text(bc_gdf, poly, years):
    series_list = []
    node_group = df_filter_rows(bc_gdf, poly, 'ID')
    for year in years:
        group_year_val = get_unique_vals(node_group, year)
        try:
            if len(group_year_val) > 1:
                with open(os.path.join(Params.outdir, 'error.log'), 'a+') as file:
                    file.write('\nTime series may not be uniquely grouped (e.g. group_0000 registers having at least one \n'
                               'node whose time series does not match the other nodes of the same group).\n'
                               'Verify boundary condition card for group ID: {} in year: {}\n'.format(poly, year))
                    print('\nTime series may not be uniquely grouped (e.g. group_0000 registers having at least one \n'
                          'node whose time series does not match the other nodes of the same group).\n'
                          'Verify boundary condition card for group ID: {} in year: {}\n'.format(poly, year))
        except Exception as e:
            with open(os.path.join(Params.outdir, 'error.log'), 'a+') as file:
                file.write('\n{}\n'.format(e))
                print(e)
        series_list += [[int(year), group_year_val[0]]]
    # Add in the final year of the simulation period
    series_list += [[Params.eos_year, series_list[-1][1]]]
    # Cleans the exhaustive listing of every year
    reduced_list = remove_excess_data(series_list)
    # Simplifies revegetation periods to use STOMP ramping
    final_series = STOMP_series_rev(reduced_list)
    # Generate text for this object
    text = 'file,../' + os.path.basename(Params.outdir) + '/' + str(poly) + '.dat,Neumann Aqueous,'
    text += str("," * Params.num_cocs) + '\n'
    text += str(len(final_series)) + ',\n'
    for point in final_series:
        year = point[0]
        val = point[1]
        text += (str(year) + ',year,-' + "%.1f" % val + ',mm/year,' + str("," * int(Params.num_cocs) * 2) + '\n')
    return text


def remove_excess_data(series_list):
    class CleanSeries:
        prev_point = series_list[0]
        prev_val = series_list[0][1]
        reduced_list = [prev_point]
    for point in series_list[1:]:
        val = point[1]
        if val == CleanSeries.prev_val:
            CleanSeries.reduced_list[-1] = point
        else:
            if CleanSeries.prev_point != CleanSeries.reduced_list[-1]:
                CleanSeries.reduced_list.insert(-1, CleanSeries.prev_point)
            CleanSeries.reduced_list += [point]
            CleanSeries.prev_point = point
        CleanSeries.prev_val = val
    # Add back in the last point that gets missed
    CleanSeries.reduced_list.insert(-1, CleanSeries.prev_point)
    return CleanSeries.reduced_list


def STOMP_series_rev(reduced_list):
    class FindLines:
        prev_year = [0]
        prev_delta = 0
        delta_index = 0
        years = {delta_index: []}
        ramp_indices = []
        final_series = []
    # To detect lines properly, need to look at points in sequence groups 3 points long
    windowed_list = zip_longest(reduced_list, reduced_list[1:], reduced_list[2:])
    for point1, point2, point3 in windowed_list:
        year1 = [point1[0]]
        year2 = [point2[0]]
        val1 = point1[1]
        val2 = point2[1]
        try:
            val3 = point3[1]
        except TypeError:
            # This will catch the end of the list, point 3 will be the first to reach a "NoneType" value
            break
        delta1 = val1 - val2
        delta2 = val2 - val3
        if check_val_proximity(delta1, delta2):
            if FindLines.delta_index not in FindLines.ramp_indices:
                FindLines.ramp_indices.append(FindLines.delta_index)
            FindLines.years[FindLines.delta_index] += year1
        elif check_val_proximity(delta1, FindLines.prev_delta) and delta1 != 0:
            FindLines.years[FindLines.delta_index] += year1
            FindLines.years[FindLines.delta_index] += year2
            FindLines.delta_index += 1
            FindLines.years[FindLines.delta_index] = []
        FindLines.prev_delta = delta1
        FindLines.prev_year = year1
    ramped_years = []
    for key in FindLines.years:
        # Keep only the start and end values of the final series (prior to creating the stepwise series) by removing
        # all but these two points from the input time series
        ramped_years += FindLines.years[key][1:-1]
    for point in reduced_list:
        year = point[0]
        if year not in ramped_years:
            FindLines.final_series.append(point)
    # Call another function to structure final series with steps in STOMP notation
    FindLines.final_series = stepped_series(FindLines)
    return FindLines.final_series


def stepped_series(FindLines):
    # This takes the series and formats it into a STOMP-readable series with ramping included
    # Initialize STOMP series variable
    stomp_series = []
    # First obtain a list of the years that are revegetation periods to prevent stepping where ramping is taking place
    reveg_years = matrix_to_list([FindLines.years[cycle] for cycle in FindLines.ramp_indices])
    for point1, point2 in zip(FindLines.final_series, FindLines.final_series[1:]):
        year1, val1 = point1[0], point1[1]
        year2, val2 = point2[0], point2[1]
        if year1 in reveg_years and year2 in reveg_years:
            stomp_series.append(point1)
            stomp_series.append(point2)
            reveg_years.remove(year1)
            reveg_years.remove(year2)
        elif val1 != val2:
            try:
                # If the last point in the series is the same value, skip the redundancy and write only the end point
                if val1 == stomp_series[-1][1]:
                    stomp_series.append([year2, val1])
                else:
                    stomp_series.append([year1, val1])
                    stomp_series.append([year2, val1])
            except:
                # In the event that the stomp_series variable is an empty list, initialize it with the first set.
                stomp_series.append([year1, val1])
                stomp_series.append([year2, val1])
        else:
            try:
                if val1 != stomp_series[-1][1]:
                    stomp_series.append(point1)
            except:
                stomp_series.append(point1)
    # Add the final point back in at the end
    stomp_series.append(FindLines.final_series[-1])
    return stomp_series


def gen_ss_file(bc_gdf):
    filepath = os.path.join(Params.outdir, 'ca_ss_boundary_card.dat')
    gen_header(filepath)
    # Verify that the node count is correct using this counter
    if not os.path.exists(os.path.join(Params.outdir)):
        os.makedirs(os.path.join(Params.outdir))
    counter = 0
    for poly in Params.unique_polys:
        selection = bc_gdf[bc_gdf['ID'] == poly]
        # Verify that the outfolder exists
        if not os.path.exists(os.path.join(Params.outdir)):
            os.makedirs(os.path.join(Params.outdir))
        with open(filepath, 'a+') as file:
            file.write('file,../' + os.path.basename(Params.outdir) + '/'
                       + str(poly) + '.dat,Neumann Aqueous,\n')
            file.write('2,\n')

            # Obtain the first year contained in the dataset
            years = []
            for col in selection.columns:
                try:
                    year = int(col)
                    years.append(year)
                except:
                    pass
            min_year = str(min(years))
            selection_fltrd = selection.loc[:, min_year]
            # Get the first index number to use, then write the file line
            for index, val in selection_fltrd.iteritems():
                file.write('0,year,-' + str(val) + ',mm/year,\n')
                file.write('10000,year,-' + str(val) + ',mm/year,\n')
                break
    print("Finished steady state boundary condition card.")
    return


def gen_integration_file(tr_card_path):
    # This will tie other functions together to manipulate the boundary condition data via a dataframe into a text file
    # Variables are initialized here:
    avgs_name = 'AVERAGES(mm/yr)'
    poly_name = 'POLY_TOTALS(m^3)'
    yearly_name = 'TOTALS(m^3)'
    cum_name = 'CUM_TOTALS(m^3)'
    node_gdf = gen_node_recs(tr_card_path)
    totals_gdf = deepcopy(node_gdf.loc[:, ['Area'] + Params.years])

    # Obtain the averages as a series object for the years of recharge values
    node_gdf.loc[avgs_name, Params.years] = node_gdf.mean()
    # Expected recharge not using any discretization
    node_gdf.loc[poly_name, Params.years] = [Params.xpctd_rchg[year] for year in Params.xpctd_rchg]
    # Obtain the yearly totals in the same fashion as the averages (divide by 1,000 in order to get in m^3)
    totals_gdf = totals_gdf.multiply(totals_gdf['Area'].div(1000, axis='index'), axis='index')
    node_gdf.loc[yearly_name, Params.years] = totals_gdf.sum()
    # Cumulative sums are calculated based on the years and the sums
    node_gdf.loc[cum_name, Params.years] = get_cum_sums(node_gdf.loc[yearly_name, Params.years])

    # Obtain the reported summary totals
    ss_per = node_gdf.loc[yearly_name, Params.years[0]] * 10000
    # Up to, but not including the year
    history_per = node_gdf.loc[cum_name, Params.his_year] - node_gdf.loc[yearly_name, Params.his_year]
    # active_per = node_gdf.loc[cum_name, Params.years[-1]] - node_gdf.loc[yearly_name, Params.years[-1]] - history_per
    post_per = node_gdf.loc[yearly_name, Params.years[-1]] * ((Params.eos_year - 1) - Params.years[-1])
    total_per = node_gdf.loc[cum_name, Params.years[-1]] + post_per
    active_per = total_per - history_per

    # Convert the dataframe to a string
    node_recs = node_gdf.to_string(
                              justify='right',
                              float_format='%.3f')
    node_recs = node_recs.split('\n')
    node_recs[0] = node_recs[0].replace(len(node_gdf.index.name) * ' ', node_gdf.index.name, 1)
    node_recs = [node_recs[0]] + node_recs[2:]
    node_recs = '\n'.join(node_recs)

    filepath = os.path.join(Params.outdir, 'ca_integration_table.dat')
    # First write the summary totals, then add the header
    with open(filepath, 'a+') as file:
        file.write("# Reported years for recharge sums follow STOMP notation where the end year listed is not\n"
                   "# included in the sum (e.g. [1943-2071] is equivalent to: JAN 1st, 1943 up to JAN 1st, 2071)\n")
        file.write("Total Recharge Volume Steady State = {:.2f} m^3\n".format(ss_per))
        file.write("Total Recharge Volume ({}-{}) = {:.2f} m^3\n".format(Params.years[0], Params.his_year, history_per))
        file.write("Total Recharge Volume ({}-{}) = {:.2f} m^3\n".format(Params.his_year, Params.eos_year, active_per))
        file.write("Total Recharge Volume ({}-{}) = {:.2f} m^3\n".format(Params.years[0], Params.eos_year, total_per))
    gen_header(filepath, integration=True)
    with open(filepath, 'a+') as file:
        file.write(node_recs)
    print("Finished generating the integration table")
    return


def get_cum_sums(totals_series):
    # Because the cumulative sum is dependent on the years specified in the corresponding column with relation to those
    # before and after, this function has to manually calculate the cumulative sums. Only if all of your yearly
    # data are sequential from year-to-year can you use the built-in Pandas "cumsum()" method
    # Add another year so that the STOMP notation is followed, important for mass balance
    totals_series = totals_series.append(
        pandas.Series([totals_series.loc[Params.years[-1]]], index=[Params.years[-1] + 1])
    )
    cum_sum_series = []
    cum_sum = 0
    for point1, point2 in zip(totals_series.iteritems(), totals_series[1:].iteritems()):
        year1, val1 = point1[0], point1[1]
        year2, val2 = point2[0], point2[1]
        cum_sum += (year2 - year1) * val1
        cum_sum_series.append(cum_sum)
    return cum_sum_series


def gen_node_recs(tr_card_path):
    # Read in the transient boundary condition card to build a new year-by-year dataframe for all nodes
    node_dict = {}
    counter = 0
    rchg_conds = 0
    recharge_series = []
    poly = ''
    node_list = []
    with open(tr_card_path, 'r') as file:
        for line in file:
            if '#' in line:
                continue
            elif 'file,' in line:
                # Extract just the polygon filename
                poly = line.split(',')[1].split('/')[-1]
                node_list = read_external_data(poly)
                poly = poly.rstrip('.dat')
                recharge_series = []
                rchg_conds = int(next(file).replace(',', ''))
            elif rchg_conds == 1:
                # From the extracted recharge time series, generate exhaustive yearly recharge values
                recharge_series.append((line.split(',')[0], abs(float(line.split(',')[2]))))
                recharge_by_year = expand_recharge_series(recharge_series)
                node_geoms = calc_node_geom('dimensions')
                for node in node_list:
                    # Using the spacing dictionary, obtain the delx, dely and area attributes for each node
                    i = node[0]
                    j = node[1]
                    k = node[2]
                    id = '_'.join([i, j])
                    node_dict[counter] = {'I': int(i),
                                          'J': int(j),
                                          'K': int(k),
                                          'ID': poly,
                                          'delX': node_geoms[id]['delx'],
                                          'delY': node_geoms[id]['dely'],
                                          'Area': node_geoms[id]['area']
                                          }
                    for point in recharge_by_year:
                        year = point[0]
                        val = float(point[1])
                        if year in Params.years:
                            node_dict[counter][year] = val
                    counter += 1
            elif rchg_conds != 0:
                recharge_series.append((line.split(',')[0], abs(float(line.split(',')[2]))))
                rchg_conds -= 1
    # Convert the dictionary to a dataframe for ease of access/calculations
    node_gdf = pandas.DataFrame.from_dict(node_dict, orient='index')
    node_gdf.sort_values(by=['I', 'J'], inplace=True)
    node_gdf = node_gdf.reindex(['I', 'J', 'K', 'ID', 'delX', 'delY', 'Area'] + Params.years, axis=1)
    node_gdf.reset_index(drop=True, inplace=True)
    node_gdf.index.name = 'Index'
    return node_gdf


def read_external_data(poly):
    # Creates a list of nodes from reading the supplied external file
    node_list = []
    with open(os.path.join(Params.outdir, poly), 'r') as ext:
        for row in ext:
            node_list.append(row.split()[:-1])
    return node_list


def expand_recharge_series(recharge_series):
    # The purpose of this function is to generate from an abbreviated time series the yearly recharge values.
    # An important aspect of this function will be that the median/mean value is selected for each year,
    # which is especially important for revegetation cycles.
    recharge_by_year = []
    for point1, point2 in zip(recharge_series, recharge_series[1:]):
        year1, val1 = int(point1[0]), float(point1[1])
        year2, val2 = int(point2[0]), float(point2[1])
        unshifted_list = np.linspace(start=val1, stop=val2, num=year2 - year1 + 1)
        # If the length is zero (i.e. No difference in years), continue to next point set
        if len(unshifted_list) == 0:
            continue
        adjusted_list = [(val1 + val2) / 2 for val1, val2 in zip(unshifted_list, unshifted_list[1:])]
        # Add the end point back in as it is truncated from the zip in the previous step
        adjusted_list += [unshifted_list[-1]]
        years = list(range(year1, year2))
        recharge_by_year += [(year, val) for year, val in zip(years, adjusted_list)]
    # Add the end point back in as it is truncated at the onset due to zipping
    recharge_by_year += [recharge_series[-1]]
    return recharge_by_year


def verification():
    '''
    This function reads the output files generated by the script to verify that each surface is uniquely defined. This
    function will also test that all of the surfaces listed in the boundary card have corresponding files and that
    all of the files are represented in the boundary conditions cards.
    '''
    # List of files to ignore when generating the list of surface definition files
    files_to_ignore = ['ca_integration_table.dat', 'ca_integration_table2.dat', 'ca_tr_boundary_card.dat', 'ca_ss_boundary_card.dat']

    # Build list of surface definition files
    surface_defs = []
    for file in next(os.walk(Params.outdir))[2]:
        if file in files_to_ignore:
            pass
        elif file[-4:] == '.dat':
            surface_defs.append(file)

    # Create lists of the external files used in the steady state and transient boundary condition cards
    ss_list = []
    with open(os.path.join(Params.outdir, 'ca_ss_boundary_card.dat'), 'r') as file:
        for line in file:
            if '.dat' in line:
                path = line.replace('\n','').replace('\r','').split(',')[1]
                surf_def = path.split('/')[-1]
                ss_list.append(surf_def)
    tr_list = []
    with open(os.path.join(Params.outdir, 'ca_tr_boundary_card.dat'), 'r') as file:
        for line in file:
            if '.dat' in line:
                path = line.split(',')[1]
                surf_def = path.split('/')[-1]
                tr_list.append(surf_def)
    # Verify that the steady-state and transient boundary cards are equivalent in their definitions of the surfaces
    if sorted(ss_list) != sorted(tr_list):
        print("Something has happened with your steady-state and transient boundary condition cards\n please verify "
              "before use in a model.")
        with open(os.path.join(Params.outdir, 'ca_integration_table.dat'), 'r') as file:
            data = file.read()
        with open(os.path.join(Params.outdir, 'ca_integration_table.dat'), 'w') as file:
            file.write("Differences were found between the surface definitions in the steady-state and transient\n"
                      "boundary cards. Strongly advise that modeler verify changes to files. Integration table is "
                      "invalid.\n")
            file.write(data)
        with open(os.path.join(Params.outdir, 'error.log'), 'a+') as file:
            file.write("Differences were found between the surface definitions in the steady-state and transient\n"
                      "boundary cards. Strongly advise that modeler verify changes to files. Integration table is "
                      "invalid.\n")

    # Verify that there are no duplicate elements in the surface files listed in the boundary condition card (because
    # steady-state and transient are equivalent, use only tr_list for further comparisons
    elif len(tr_list) != len(set(tr_list)):
        print("The same file defining a set of surfaces for STOMP has been used more than once.\n modeler should NOT "
              "use these files for simulation in STOMP.")
        with open(os.path.join(Params.outdir, 'error.log'), 'a+') as file:
            file.write('Duplicate listings of the same surfaces for STOMP were found in the boundary condition cards.\n'
                       'Modeler advised to not use the boundary condition cards or integration table.')

    # Verify that the list of files originally obtained matches the list of files written out
    elif sorted(tr_list) != sorted(surface_defs):
        print("The written surface definitions in the directory do not match those used in the boundary cards.\n"
              "Modeler is strongly advised to not use the boundary condition cards")
        with open(os.path.join(Params.outdir, 'error.log'), 'a+') as file:
            file.write('The written surface definitions in the directory do not match those used in the boundary cards.'
                       '\nModeler is strongly advised to not use the boundary condition cards')

    # Verify that the nodes listed in the external files are all unique and match the expected number of nodes
    else:
        nodes = {}
        for surface in surface_defs:
            with open(os.path.join(Params.outdir, surface), 'r') as file:
                for line in file:
                    line = list(map(str, line.split()))
                    i, j, k = line[0], line[1], line[2]
                    id = '{0}_{1}'.format(int(float(i)), int(float(j)))
                    if id not in nodes:
                        nodes[id] = {"K": k, "file": surface}
                    else:
                        nodes['error'] = {'file': surface}
                        print("You have repeated node " + id + " in files '" + surface + "', '" + nodes[id]['file'] +
                              "'\n")
                        with open(os.path.join(Params.outdir, 'error.log'), 'a+') as file:
                            file.write("You have repeated node " + id + " in files '" + surface + "', '" +
                                       nodes[id]['file'] + "'\n")
        if len(nodes) != Params.node_count:
            print("Your *.dat files containing the surface information have a total of " + str(len(nodes)) +
                  "nodes. The expected number of nodes is " + str(Params.node_count))
            with open(os.path.join(Params.outdir, 'error.log'), 'a+') as file:
                file.write("Your *.dat files containing the surface information have a total of " + str(len(nodes)) +
                  " nodes. The expected number of nodes is " + str(Params.node_count) + '.\n')
        else:
            # Final check is to verify that the nodes in the dictionary match those in the integration table
            check_nodes = {}
            check_surfs = []
            with open(os.path.join(Params.outdir, 'ca_integration_table.dat'), 'r') as file:
                for line in file:
                    if '#' in line or 'Total' in line or 'ID' in line or 'NaN' in line:
                        continue
                    else:
                        line = list(map(str, line.split()))
                        i, j, surf = line[1], line[2], line[4]
                        id = '{0}_{1}'.format(int(float(i)), int(float(j)))
                        if id not in check_nodes:
                            check_nodes[id] = {'file': surf}
                            check_surfs.append(surf + '.dat')
                        else:
                            check_nodes['error_int'] = {'file': surf}
                            print("The integration table has become corrupted and contains multiple definitions of the\n"
                                  "same node. Modeler advised to not use the files from this RET output set.")
                            with open(os.path.join(Params.outdir, 'error.log'), 'a+') as error_log:
                                error_log.write("The integration table has become corrupted and contains multiple " +
                                                "definitions of the\nsame node. Modeler advised to not use the files" +
                                                " from this RET output set.")
            if sorted(set(check_surfs)) != sorted(tr_list):
                print("Surface files listed in the integration table do not match boundary condition cards.\n" +
                      "Integration table is invalid.")
                with open(os.path.join(Params.outdir, 'ca_integration_table.dat'), 'r') as file:
                    data = file.read()
                with open(os.path.join(Params.outdir, 'ca_integration_table.dat'), 'w') as file:
                    file.write('#**\n#Surface files listed in the integration table do not match boundary condition ' +
                               'cards.\n#Integration table is invalid.\n#**\n')
                    file.write(data)
            elif sorted(check_nodes.keys()) == sorted(nodes.keys()):
                with open(os.path.join(Params.outdir, 'ca_integration_table.dat'), 'r') as file:
                    data = file.read()
                with open(os.path.join(Params.outdir, 'ca_integration_table.dat'), 'w') as file:
                    file.write("#**\n#Surfaces listed in boundary condition cards and recorded external files are uniquely " +
                               "defined.\n#The node definitions provided in the integration table correspond with both " +
                               "boundary condition\n#input cards and external files referenced.\n#**\n")
                    file.write(data)
            else:
                print("The surface definitions do not match between the integration table and boundary cards/external" +
                      " files.\nModeler advised to not use integration table in conjunction with boundary cards.")
                with open(os.path.join(Params.outdir, 'error.log'), 'a+') as error_log:
                    error_log.write("The surface definitions do not match between the integration table and boundary " +
                                    "cards/external files.\nModeler advised to not use integration table in " +
                                    "conjunction with boundary cards.")
    return


def gen_ts_ids(gdf):
    # This function's intended purpose is to create identifiers for unique time series
    # gdf is a geodataframe expected to have numeric columns from which a time series can be extracted for each I, J, K
    # Example:
    #   I   J   K   1943    1944    1945
    #   1   1   1    1.5    46.0    46.0
    import hashlib
    # Obtain the columns pertinent for the time series, also record the columns that are the final column set
    numeric_cols = get_int_cols(gdf)
    numeric_cols.sort()
    numeric_cols = list(map(str, numeric_cols))
    keep_cols = gdf.columns.to_list()

    # Add a key column to be used later for mapping hashes back for unique identifiers
    gdf['I_J_K'] = gdf['I'].astype(str) + '_' + gdf['J'].astype(str) + '_' + gdf['K'].astype(str)

    # Build a dictionary of the hashes, then map it back as a new column to the geodataframe
    hash_dict = {}
    for index, row in gdf[numeric_cols + ['I_J_K']].iterrows():
        val_list = str(row[numeric_cols].values.tolist())
        hash_dict[row['I_J_K']] = hashlib.sha256(val_list.encode()).hexdigest()

    # Map hashes to column in dataframe
    gdf['hash_id'] = gdf['I_J_K'].map(hash_dict)

    # Sort the dataframe from low to high recharge through time, generate unique ID's and return the gdf with the
    # modified ID column
    gdf = gdf.sort_values(by=numeric_cols)
    hash_ids = gdf['hash_id'].unique()
    hash_id_dict = {hash: 'group_{:05d}'.format(i) for hash, i in zip(hash_ids, range(len(hash_ids)))}
    gdf['ID'] = gdf['hash_id'].map(hash_id_dict)
    gdf = gdf[keep_cols]
    return gdf


# ----------------------------------------------------------------------------------------------------------------------
# Main Program
if __name__=='__main__':
    # Set variables in queue objects to preserve the data properly
    bc_gdf_dat, recharge_list_dat, grid_surfs_dat = get_parameters()
    bc_gdf_dat.to_file(os.path.join(Params.outdir, 'bc_gdf'))
    grid_surfs_dat.to_file(os.path.join(Params.outdir, 'grid_surfs'))

    # Set parameters from data
    Params.years = get_int_cols(bc_gdf_dat)
    Params.unique_polys = get_unique_vals(bc_gdf_dat, 'ID')

    # Generate the final product
    tr_file_path = gen_tr_file(bc_gdf_dat)
    Params.num_nodes = gen_externals(bc_gdf_dat)
    gen_ss_file(bc_gdf_dat)
    gen_integration_file(tr_file_path)
    verification()
